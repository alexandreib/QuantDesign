{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexandreib/QuantDesign/blob/main/SP500_Portfolio_Allocation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "#**Portfolio Optimization on S&P 500 Stocks**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "import seaborn as sns\n",
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "m0-7ZhGc9ERF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the Stocks Returns\n"
      ],
      "metadata": {
        "id": "OkjUw457cLUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping SP500 Constituents"
      ],
      "metadata": {
        "id": "pkLVg3Cw4YIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the S&P 500 constituents from Wikipedia\n",
        "try:\n",
        "    table=pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
        "    df = table[0]\n",
        "    tickers = df['Symbol'].tolist()\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading S&P 500 tickers: {e}\")\n",
        "    tickers = [] # Handle the error gracefully, e.g., provide a default list\n",
        "\n",
        "# Print or use the tickers list\n",
        "print(tickers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9J52CMM8XZA",
        "outputId": "e0b00132-835b-45e7-96d4-8d520db3a44b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['MMM', 'AOS', 'ABT', 'ABBV', 'ACN', 'ADBE', 'AMD', 'AES', 'AFL', 'A', 'APD', 'ABNB', 'AKAM', 'ALB', 'ARE', 'ALGN', 'ALLE', 'LNT', 'ALL', 'GOOGL', 'GOOG', 'MO', 'AMZN', 'AMCR', 'AMTM', 'AEE', 'AEP', 'AXP', 'AIG', 'AMT', 'AWK', 'AMP', 'AME', 'AMGN', 'APH', 'ADI', 'ANSS', 'AON', 'APA', 'AAPL', 'AMAT', 'APTV', 'ACGL', 'ADM', 'ANET', 'AJG', 'AIZ', 'T', 'ATO', 'ADSK', 'ADP', 'AZO', 'AVB', 'AVY', 'AXON', 'BKR', 'BALL', 'BAC', 'BAX', 'BDX', 'BRK.B', 'BBY', 'TECH', 'BIIB', 'BLK', 'BX', 'BK', 'BA', 'BKNG', 'BWA', 'BSX', 'BMY', 'AVGO', 'BR', 'BRO', 'BF.B', 'BLDR', 'BG', 'BXP', 'CHRW', 'CDNS', 'CZR', 'CPT', 'CPB', 'COF', 'CAH', 'KMX', 'CCL', 'CARR', 'CTLT', 'CAT', 'CBOE', 'CBRE', 'CDW', 'CE', 'COR', 'CNC', 'CNP', 'CF', 'CRL', 'SCHW', 'CHTR', 'CVX', 'CMG', 'CB', 'CHD', 'CI', 'CINF', 'CTAS', 'CSCO', 'C', 'CFG', 'CLX', 'CME', 'CMS', 'KO', 'CTSH', 'CL', 'CMCSA', 'CAG', 'COP', 'ED', 'STZ', 'CEG', 'COO', 'CPRT', 'GLW', 'CPAY', 'CTVA', 'CSGP', 'COST', 'CTRA', 'CRWD', 'CCI', 'CSX', 'CMI', 'CVS', 'DHR', 'DRI', 'DVA', 'DAY', 'DECK', 'DE', 'DELL', 'DAL', 'DVN', 'DXCM', 'FANG', 'DLR', 'DFS', 'DG', 'DLTR', 'D', 'DPZ', 'DOV', 'DOW', 'DHI', 'DTE', 'DUK', 'DD', 'EMN', 'ETN', 'EBAY', 'ECL', 'EIX', 'EW', 'EA', 'ELV', 'EMR', 'ENPH', 'ETR', 'EOG', 'EPAM', 'EQT', 'EFX', 'EQIX', 'EQR', 'ERIE', 'ESS', 'EL', 'EG', 'EVRG', 'ES', 'EXC', 'EXPE', 'EXPD', 'EXR', 'XOM', 'FFIV', 'FDS', 'FICO', 'FAST', 'FRT', 'FDX', 'FIS', 'FITB', 'FSLR', 'FE', 'FI', 'FMC', 'F', 'FTNT', 'FTV', 'FOXA', 'FOX', 'BEN', 'FCX', 'GRMN', 'IT', 'GE', 'GEHC', 'GEV', 'GEN', 'GNRC', 'GD', 'GIS', 'GM', 'GPC', 'GILD', 'GPN', 'GL', 'GDDY', 'GS', 'HAL', 'HIG', 'HAS', 'HCA', 'DOC', 'HSIC', 'HSY', 'HES', 'HPE', 'HLT', 'HOLX', 'HD', 'HON', 'HRL', 'HST', 'HWM', 'HPQ', 'HUBB', 'HUM', 'HBAN', 'HII', 'IBM', 'IEX', 'IDXX', 'ITW', 'INCY', 'IR', 'PODD', 'INTC', 'ICE', 'IFF', 'IP', 'IPG', 'INTU', 'ISRG', 'IVZ', 'INVH', 'IQV', 'IRM', 'JBHT', 'JBL', 'JKHY', 'J', 'JNJ', 'JCI', 'JPM', 'JNPR', 'K', 'KVUE', 'KDP', 'KEY', 'KEYS', 'KMB', 'KIM', 'KMI', 'KKR', 'KLAC', 'KHC', 'KR', 'LHX', 'LH', 'LRCX', 'LW', 'LVS', 'LDOS', 'LEN', 'LLY', 'LIN', 'LYV', 'LKQ', 'LMT', 'L', 'LOW', 'LULU', 'LYB', 'MTB', 'MRO', 'MPC', 'MKTX', 'MAR', 'MMC', 'MLM', 'MAS', 'MA', 'MTCH', 'MKC', 'MCD', 'MCK', 'MDT', 'MRK', 'META', 'MET', 'MTD', 'MGM', 'MCHP', 'MU', 'MSFT', 'MAA', 'MRNA', 'MHK', 'MOH', 'TAP', 'MDLZ', 'MPWR', 'MNST', 'MCO', 'MS', 'MOS', 'MSI', 'MSCI', 'NDAQ', 'NTAP', 'NFLX', 'NEM', 'NWSA', 'NWS', 'NEE', 'NKE', 'NI', 'NDSN', 'NSC', 'NTRS', 'NOC', 'NCLH', 'NRG', 'NUE', 'NVDA', 'NVR', 'NXPI', 'ORLY', 'OXY', 'ODFL', 'OMC', 'ON', 'OKE', 'ORCL', 'OTIS', 'PCAR', 'PKG', 'PLTR', 'PANW', 'PARA', 'PH', 'PAYX', 'PAYC', 'PYPL', 'PNR', 'PEP', 'PFE', 'PCG', 'PM', 'PSX', 'PNW', 'PNC', 'POOL', 'PPG', 'PPL', 'PFG', 'PG', 'PGR', 'PLD', 'PRU', 'PEG', 'PTC', 'PSA', 'PHM', 'QRVO', 'PWR', 'QCOM', 'DGX', 'RL', 'RJF', 'RTX', 'O', 'REG', 'REGN', 'RF', 'RSG', 'RMD', 'RVTY', 'ROK', 'ROL', 'ROP', 'ROST', 'RCL', 'SPGI', 'CRM', 'SBAC', 'SLB', 'STX', 'SRE', 'NOW', 'SHW', 'SPG', 'SWKS', 'SJM', 'SW', 'SNA', 'SOLV', 'SO', 'LUV', 'SWK', 'SBUX', 'STT', 'STLD', 'STE', 'SYK', 'SMCI', 'SYF', 'SNPS', 'SYY', 'TMUS', 'TROW', 'TTWO', 'TPR', 'TRGP', 'TGT', 'TEL', 'TDY', 'TFX', 'TER', 'TSLA', 'TXN', 'TXT', 'TMO', 'TJX', 'TSCO', 'TT', 'TDG', 'TRV', 'TRMB', 'TFC', 'TYL', 'TSN', 'USB', 'UBER', 'UDR', 'ULTA', 'UNP', 'UAL', 'UPS', 'URI', 'UNH', 'UHS', 'VLO', 'VTR', 'VLTO', 'VRSN', 'VRSK', 'VZ', 'VRTX', 'VTRS', 'VICI', 'V', 'VST', 'VMC', 'WRB', 'GWW', 'WAB', 'WBA', 'WMT', 'DIS', 'WBD', 'WM', 'WAT', 'WEC', 'WFC', 'WELL', 'WST', 'WDC', 'WY', 'WMB', 'WTW', 'WYNN', 'XEL', 'XYL', 'YUM', 'ZBRA', 'ZBH', 'ZTS']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the last 10 years of price"
      ],
      "metadata": {
        "id": "5IFwPZQOb3pw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the start and end dates for the data\n",
        "end_date = pd.Timestamp.today()\n",
        "start_date = end_date - pd.Timedelta(days=365 * 10)\n",
        "\n",
        "# Download the data\n",
        "df = yf.download(tickers, start=start_date, end=end_date)\n",
        "\n",
        "# Print the data (optional)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpOPEOj-8QNv",
        "outputId": "e9e1dd95-b41b-4970-cd0c-44cc751b3e94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[                       1%                       ]  7 of 503 completed"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reshape, and Clean the DataFrame"
      ],
      "metadata": {
        "id": "RQSe7eYLcAZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape the DataFrame\n",
        "df = df['Adj Close'].reset_index()\n",
        "df = pd.melt(df, id_vars='Date', value_vars=tickers, var_name='Ticker', value_name='Adj Close')\n",
        "df['Date'] = pd.to_datetime(df['Date']).dt.date\n",
        "df = df.rename(columns={'index': 'Ticker'})\n",
        "\n",
        "# Print or use the transformed DataFrame\n",
        "df.head()"
      ],
      "metadata": {
        "id": "t7GbgtEa9gI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the Returns"
      ],
      "metadata": {
        "id": "mDBKPl2PAJVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Log Returns\n",
        "\n",
        "Using log returns instead of simple returns has several advantages, especially in financial analysis:\n",
        "\n",
        "1. **Time Additivity**\n",
        "Log returns are time-additive, meaning that you can sum them over different periods to get the total return. For example, if you have daily log returns, you can sum them to find the cumulative return over a week or month.\n",
        "Simple returns, on the other hand, do not have this property and require a more complex calculation for cumulative returns.\n",
        "2. **Handling Compounding**\n",
        "Log returns account for compounding naturally. When returns are compounded, log returns provide a more accurate measure of the growth of an investment over time.\n",
        "Simple returns can underestimate or overestimate returns if not compounded properly.\n",
        "3. **Normality Assumption**\n",
        "Log returns tend to be more normally distributed than simple returns, particularly for large datasets. This property is useful for statistical modeling and risk management.\n",
        "4. **Symmetry**\n",
        "Log returns treat gains and losses symmetrically. A 10% gain followed by a 10% loss results in a net loss in simple returns, but the log returns more accurately reflect the continuous nature of returns.\n",
        "5. **Ease of Analysis**\n",
        "Many financial models, like the Black-Scholes option pricing model, rely on the normal distribution, making log returns more compatible with such models.\n",
        "\n",
        "**Conclusion** <br>\n",
        "While simple returns are easier to calculate and interpret for short-term analyses, log returns are generally more robust and useful for long-term investments and statistical analyses, especially when compounding and time periods are involved."
      ],
      "metadata": {
        "id": "LawbuQHVbeUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate daily log returns\n",
        "df['Daily_Log_Return'] = np.log(df['Adj Close'] / df['Adj Close'].shift(1))\n",
        "\n",
        "# # Calculate quarterly log returns Using the first advantage of log Rturns : Tie Additivity\n",
        "df['Quarterly_Log_Return'] = df.groupby('Ticker')['Daily_Log_Return'].rolling(window=63, min_periods=1).sum().reset_index(0,drop=True)\n",
        "\n",
        "# Creation of the log return Matrix\n",
        "log_returns_matrix = pd.pivot_table(df[['Date','Ticker', 'Quarterly_Log_Return']], index ='Date', columns = 'Ticker', aggfunc='mean')\n",
        "log_returns_matrix = log_returns_matrix.droplevel(0, axis = 'columns')\n",
        "log_returns_matrix.head(1)\n"
      ],
      "metadata": {
        "id": "7d3K3D3Z_HU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shrinked Covariance Matrice\n",
        "\n",
        "In situations with many variables and skewed data, using a shrinkage covariance matrix is typically the better choice.\n",
        "Here's why, we will use the Shrinkage Covariance Matrix :\n",
        "\n",
        "1.  **High Dimensionality:** When the number of variables is large relative to the number of observations, the standard covariance matrix can become unstable and lead to overfitting. Shrinkage helps to stabilize these estimates.\n",
        "\n",
        "2.   **Skewed Data:** Skewed data can affect the estimation of the covariance matrix. Shrinkage techniques can provide more robust estimates by incorporating prior information or adjusting the influence of extreme values.\n",
        "\n",
        "3.   **Variance Reduction:** Shrinkage methods reduce the variance of the covariance estimates, which is particularly beneficial in high dimensions. This can improve the reliability of downstream analyses, such as portfolio optimization or classification.\n",
        "\n",
        "4.  **Bias-Variance Trade-off:** While shrinkage introduces some bias, it often results in a lower mean squared error compared to the traditional covariance matrix, especially in high-dimensional contexts.\n",
        "\n",
        "**Conclusion** <br>\n",
        "Given the presence of many variables and skewed data, a shrunk covariance matrix is generally more effective, as it mitigates the instability and variability issues that arise in such scenarios. Techniques like Ledoit-Wolf shrinkage or other regularization methods can be particularly useful."
      ],
      "metadata": {
        "id": "L3mLBYjbQhWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_shrink_cov_matrix (df) :\n",
        "    masked_arr = np.ma.array(df, mask=np.isnan(df))\n",
        "    cov_numpy = np.ma.cov(masked_arr, rowvar=False, allow_masked=True, ddof=1).data\n",
        "    n_samples, n_features = df.shape\n",
        "    alpha = np.mean(cov_numpy**2)\n",
        "    mu = np.trace(cov_numpy) / n_features\n",
        "    mu_squared = mu**2\n",
        "    num = alpha + mu_squared\n",
        "    den = (n_samples + 1) * (alpha - mu_squared / n_features)\n",
        "    shrinkage = 1.0 if den == 0 else min(num / den, 1.0)\n",
        "    shrunk_cov = (1.0 - shrinkage) * cov_numpy\n",
        "    shrunk_cov.flat[:: n_features + 1] += shrinkage * mu\n",
        "    return shrunk_cov\n",
        "\n",
        "covariance_matrix = calculate_shrink_cov_matrix(log_returns_matrix)\n",
        "\n"
      ],
      "metadata": {
        "id": "Q2We4QxOBliY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare data\n",
        "We will take for returns only the positive past quarter returns (it's safe assumption that we will not invest in stocks which we assume will have negative returns).\n",
        "\n",
        "We take the last quarter return, considering the momentum is strong enough to have similars returns the next quarter.\n",
        "\n",
        "Let do the random allocation again with only the positive expected returns.\n"
      ],
      "metadata": {
        "id": "AfnZxLYUIQlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter returns\n",
        "log_returns = log_returns_matrix.iloc[-1]\n",
        "log_returns = log_returns[log_returns > 0]\n",
        "\n",
        "#get list of tickers\n",
        "l_tickers = list(log_returns.index)\n",
        "\n",
        "number_of_tickers = len(l_tickers)\n",
        "# Out of Curiositry how many stock where with positive returns in the SP500 in the last quarter :\n",
        "print(f\"number_of_tickers with postivie returns in the last quarter : {number_of_tickers}\")\n",
        "\n",
        "# Store the individual stock standard deviation for future use\n",
        "std_dev_individual_stocks = log_returns_matrix[l_tickers].std()\n",
        "\n",
        "# Recalculate teh covariance Matrice\n",
        "covariance_matrix = calculate_shrink_cov_matrix(log_returns_matrix[l_tickers])"
      ],
      "metadata": {
        "id": "h-rSRVKoIJZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Portfolio Allocation"
      ],
      "metadata": {
        "id": "tEls-jQfeGdu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Portfolio allocation"
      ],
      "metadata": {
        "id": "sigXBXq9BmxQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next paragraph we will randomly assign weight to the different stocks, and calcualte the return, standard deviation, and sharpe ratio.\n",
        "\n",
        "We do that for 5000 simulations and plot the results."
      ],
      "metadata": {
        "id": "c6ocIz-PCWuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l_results = []\n",
        "for idx in range(5000):\n",
        "    weights = np.array(np.random.random(number_of_tickers))\n",
        "    weights = weights / np.sum(weights) ## weights total < 1\n",
        "\n",
        "    Returns = np.sum(log_returns * weights)\n",
        "    Standard_Deviation = np.sqrt(np.dot(weights.T, np.dot(covariance_matrix, weights)))\n",
        "    Sharpe_Ratio = Returns/Standard_Deviation\n",
        "    Model = 'Random allocation'\n",
        "    l_results.append([Returns, Standard_Deviation, Sharpe_Ratio, Model])\n",
        "\n",
        "results = pd.DataFrame(l_results, columns = ['Return', 'Standard Deviation', 'Sharpe Ratio', 'Model'])\n",
        "\n",
        "sns.scatterplot(results, x='Standard Deviation',  y='Return', hue = 'Sharpe Ratio')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XT1Z7cLKA9BP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the average Sharpe ratio, return and standard deviation in the above simulation"
      ],
      "metadata": {
        "id": "3znI9yoI5Jio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_mean = results[['Return', 'Standard Deviation', 'Sharpe Ratio']].mean()\n",
        "display(results_mean)"
      ],
      "metadata": {
        "id": "mfedhppt3KjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The random weight allocation portfolio demonstrate significant variability, highlighting the unpredictability of returns without a strategic approach. While it serves as a useful benchmark, it underscores the importance of informed asset selection to achieve consistent performance and risk management."
      ],
      "metadata": {
        "id": "WcaynxUh5nEE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimized Portfolio Allocation"
      ],
      "metadata": {
        "id": "Mxp1-o3qGY7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mean Variance Optimization :**\n",
        "\n",
        " $\\qquad$ Mean-variance portfolio optimization is a method for constructing portfolios that maximize returns for a given risk level or, conversely, minimize risk for a given expected return. It balances the trade-off between return and risk by optimizing asset weights in a portfolio.\n",
        "\n",
        "The optimization problem can be formulated as:\n",
        "\n",
        "- **Objective, Maximize : $$R_p - \\lambda \\cdot \\sigma_p^2$$**\n",
        "\n",
        "where:\n",
        "- - $R_p = \\sum_{i=1}^{n} w_i \\cdot R_i$ Portfolio return.\n",
        "    - $w$ : Weight of the portfolio\n",
        "    - $R_i$ : Return of invdividual stock\n",
        "    \n",
        "- $\\sigma_p^2 = w^T \\Sigma w $ : Portfolio variance (risk).\n",
        "    - $\\Sigma$ : Covariance Matrice\n",
        "- $\\lambda $ : Risk aversion parameter, where higher values prioritize minimizing risk.\n",
        "\n",
        "Solving this problem yields the optimal asset weights, creating a portfolio on the **efficient frontier**—the set of portfolios with the best return-risk trade-off.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Sharpe Ratio Optimization :**\n",
        "\n",
        "\n",
        " $\\qquad$ The Sharpe Ratio is a metric that helps investors understand the return on an investment relative to its risk. It considers not just how much an investment is expected to earn but also how much volatility or risk is involved to achieve those returns. A higher Sharpe Ratio indicates a more attractive investment, as it suggests that the investment provides better returns for the amount of risk taken. This ratio is widely used because it enables comparison across different investments, helping investors identify those that are likely to offer the best risk-adjusted returns.\n",
        "\n",
        "To optimize the Sharpe ratio, you typically want to maximize the following objective function:\n",
        "\n",
        "$$\n",
        "S = \\frac{R_p - R_f}{\\sigma_p}\n",
        "$$\n",
        "- **Where**:\n",
        "  - $S$ : Sharpe ratio\n",
        "  - $R_p$ = Portfolio return\n",
        "  - $R_f$ = Risk-free rate (to be defined)\n",
        "  - $\\sigma_p$ = Portfolio standard deviation (risk)\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "The optimization problem can be framed as follows:\n",
        "\n",
        "**Objective, Maximize :**$$S = \\frac{\\sum_{i=1}^{n} w_i \\cdot R_i - R_f}{\\sqrt{w^T \\Sigma w}}$$\n",
        "\n",
        "**Subject to**:\n",
        "- $\\sum_{i=1}^{n} w_i = 1 $\n",
        "- $w_i \\geq 0 $\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "As it's difficult problem to resolve with a simple solver, we can have 3 ways to optimize the sharpe ratio :\n",
        "\n",
        "1.   Following Tütüncü (section 5.2), \"Advanced Lecture on Mathematical Science and Information Science I\" this can be reformulated under a change of variables to a simpler quadratic optimisation problemm.\n",
        "\n",
        "    **Minimize: $ y^TQy$**\n",
        "    \n",
        "    **Subject to**:\n",
        "    - $\\hat{μ}Ty=1$\n",
        "    - $\\hat{A}y \\geq 0$\n",
        "    - $ 0 \\leq y$\n",
        "\n",
        "\n",
        "\n",
        "2.   Use Minimum Variance portfolio optimization and plot the efficient frontier, pick the point which is graphically the best Sharpe Ratio.\n",
        "\n",
        "3. Use a nonlinear programming (NLP) solve (scipy)."
      ],
      "metadata": {
        "id": "0HdOtxMUfixA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objectives functions"
      ],
      "metadata": {
        "id": "AI7NRV0juFG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scipy.optimize.minimize function is useful for finding the minimum of scalar functions with or without constraints.\n",
        "\n",
        "[Link to scipy library](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html)"
      ],
      "metadata": {
        "id": "FFEhnicll1Lt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_portfolio_variance(weights, cov_matrix):\n",
        "    return np.dot(weights.T, np.dot(cov_matrix, weights))\n",
        "\n",
        "def calculate_portfolio_returns(weights, returns):\n",
        "    return np.dot(weights, returns)\n",
        "\n",
        "def variance_objective(weights, cov_matrix):\n",
        "    portfolio_variance = np.squeeze(calculate_portfolio_variance(weights, cov_matrix))\n",
        "    return portfolio_variance\n",
        "\n",
        "### Using Negative Markowitz Objective, as we will use scipy.optimize.minimize\n",
        "def neg_markowitz_objective(weights, returns, cov_matrix, gamma = 0.2):\n",
        "    portfolio_returns = np.squeeze(calculate_portfolio_returns(weights, returns))\n",
        "    portfolio_variance = np.squeeze(calculate_portfolio_variance(weights, cov_matrix))\n",
        "    return gamma * portfolio_variance - portfolio_returns\n",
        "\n",
        "### Using Negative Sharpe Ratio, as we will use scipy.optimize.minimize\n",
        "def neg_sharpe_ratio_objective(weights, returns, cov_matrix, risk_free_rate = 0.03):\n",
        "    portfolio_returns = np.squeeze(calculate_portfolio_returns(weights, returns))\n",
        "    portfolio_variance = np.squeeze(calculate_portfolio_variance(weights, cov_matrix))\n",
        "    return -((portfolio_returns - risk_free_rate)/np.sqrt(portfolio_variance))"
      ],
      "metadata": {
        "id": "VsNlh3F65NfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1. Maximize : Mean - Variance**\n",
        "\n",
        " $\\qquad$ The Mean-Variance Portfolio approach focuses on creating a balance between expected return and risk. By carefully selecting and weighting different assets, this method aims to maximize returns for a specified level of risk or, alternatively, minimize risk for a target return. It considers both the individual performance of assets and how they interact with each other, accounting for factors like correlation and volatility. In essence, the Mean-Variance Portfolio seeks an optimal mix of assets that offers the best possible return given the level of risk an investor is willing to accept, forming the basis for efficient, diversified investing."
      ],
      "metadata": {
        "id": "kWrnknsWv2LE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = sp.optimize.minimize(fun=neg_markowitz_objective,\n",
        "                            args=(log_returns, covariance_matrix),\n",
        "                            x0= np.array([1/number_of_tickers for _ in range(number_of_tickers)]),\n",
        "                            method='SLSQP',\n",
        "                            bounds=tuple((0, 0.3) for _ in range(number_of_tickers)),## we don't want to have more than 30% of the portfolio on 1 asset\n",
        "                            constraints=({'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1})\n",
        "                            )\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "vYC0kwIJ5Uwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The optimization ended positively, let's print the results, and largest weights (above 0.01) :"
      ],
      "metadata": {
        "id": "aTalkDAr54gC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "markowitz = -1 * result['fun']\n",
        "\n",
        "weights =  result['x']\n",
        "portfolio_return = weights @ log_returns\n",
        "portfolio_std_dev = np.sqrt(weights.T @ covariance_matrix @ weights)\n",
        "sharpe_ratio = portfolio_return / portfolio_std_dev\n",
        "\n",
        "results = pd.concat([results, pd.DataFrame([[weights @ log_returns,  portfolio_std_dev, sharpe_ratio, 'Mean Variance optimized']], columns = results.columns)])\n",
        "print(f\"Sharpe Ratio: {sharpe_ratio:.3f}\")\n",
        "print(f\"Portfolio Return: {portfolio_return:.3f}\")\n",
        "print(f\"Portfolio Standard Deviation: {portfolio_std_dev:.3f}\")\n",
        "\n",
        "# Let's plot the Weights > 0.01\n",
        "sns.barplot(y=weights[weights >= 0.01], x = list(log_returns.index[weights >= 0.01]))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z8L9BBsz3o0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Minimize :** $ y^TQy$\n"
      ],
      "metadata": {
        "id": "1AcG9Htht-Yi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TBD"
      ],
      "metadata": {
        "id": "cGM_vvc32t20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3. Minimize : Variance**\n",
        "\n",
        " $\\qquad$ The Minimum Variance Portfolio is designed to minimize overall risk by carefully balancing asset weights, regardless of expected return. Instead of focusing on maximizing profits, this portfolio approach seeks to create the least risky combination of assets. It does this by including assets that, when combined, offset each other's price movements—essentially reducing the portfolio’s sensitivity to market volatility. The result is a portfolio that prioritizes stability and seeks the lowest possible risk while remaining invested in a diverse set of assets."
      ],
      "metadata": {
        "id": "wWIL-srMVW_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = sp.optimize.minimize(fun=calculate_portfolio_variance,\n",
        "                                x0 = np.array([1/number_of_tickers for _ in range(number_of_tickers)]),\n",
        "                                args=(covariance_matrix),\n",
        "                                method='SLSQP',\n",
        "                                bounds=tuple((0, 0.3) for _ in range(len(log_returns))),\n",
        "                                constraints=({'type': 'eq', 'fun': lambda x: np.sum(x) - 1}))"
      ],
      "metadata": {
        "id": "G_gDJkjPVRrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The optimization ended positively, let's print the results, and largest weights (above 0.01) :"
      ],
      "metadata": {
        "id": "IjhXA3f16FkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "variance = -1 * result['fun']\n",
        "weights =  result['x']\n",
        "portfolio_return = weights @ log_returns\n",
        "portfolio_std_dev = np.sqrt(weights.T @ covariance_matrix @ weights)\n",
        "sharpe_ratio = portfolio_return / portfolio_std_dev # calculation the same sharpe ratio for all models\n",
        "\n",
        "results = pd.concat([results, pd.DataFrame([[weights @ log_returns,  portfolio_std_dev, sharpe_ratio, 'Variance optimized']], columns = results.columns)])\n",
        "print(f\"Sharpe Ratio: {sharpe_ratio:.3f}\")\n",
        "print(f\"Portfolio Return: {portfolio_return * 100:.1f} % \")\n",
        "print(f\"Portfolio Standard Deviation: {portfolio_std_dev:.3f}\")\n",
        "\n",
        "# Let's plot the Weights > 0.01\n",
        "sns.barplot(y=weights[weights >= 0.01], x = list(log_returns.index[weights >= 0.01]))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mr_PsDASep50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Efficiency Frontier***\n",
        "\n",
        " $\\qquad$ The Efficient Frontier represents the set of optimal portfolios that offer the best possible return for a given level of risk. Imagine it as a curve that plots the most efficient portfolios from all possible combinations of assets—where each point on this curve reflects a unique risk-return balance. Portfolios below the frontier are suboptimal, as they either take on too much risk for their returns or achieve lower returns than possible for their risk level. By choosing a portfolio on the Efficient Frontier, investors can ensure they’re maximizing returns without taking unnecessary risks, making it a key concept in portfolio optimization."
      ],
      "metadata": {
        "id": "gNpD6Qi5rmhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "std_dev = []\n",
        "returns = []\n",
        "sr = []\n",
        "model = []\n",
        "\n",
        "for i, one_return in enumerate(np.linspace(portfolio_return, log_returns.max()+0.05, 20)):\n",
        "    result = sp.optimize.minimize(fun=calculate_portfolio_variance,\n",
        "                                x0 = np.array([1/number_of_tickers for _ in range(number_of_tickers)]),\n",
        "                                args=(covariance_matrix),\n",
        "                                  method='SLSQP',\n",
        "                                  bounds=tuple((0, min(1, i/25 + 0.3)) for _ in range(len(log_returns))),\n",
        "                                  constraints=({'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1},\n",
        "                                               {'type': 'eq', 'fun': lambda weights: np.dot(weights, log_returns) - one_return}))\n",
        "\n",
        "    ## When allocate all weight to only one stock replace the covariance by the variance of that stock\n",
        "    if len(np.array(l_tickers)[result['x']> 0.4]) == 1 :\n",
        "        std_dev.append(std_dev_individual_stocks[std_dev_individual_stocks.index == np.squeeze(np.array(l_tickers)[result['x']> 0.4])].values[0])\n",
        "    else :\n",
        "        std_dev.append(np.sqrt(result['fun']))\n",
        "\n",
        "    returns.append(np.dot( result['x'], log_returns))\n",
        "    sr.append(np.dot( result['x'], log_returns) / np.sqrt(result['fun']))\n",
        "    model.append('Efficiency Frontier')"
      ],
      "metadata": {
        "id": "y5gB0ej1rlg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "efficient_frontier = pd.DataFrame({'Return':returns, 'Standard Deviation':std_dev, 'Sharpe Ratio':sr, 'Model':model})\n",
        "sns.lineplot(efficient_frontier, y='Return', x='Standard Deviation')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nMSUWOctuaPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Maximize : Sharpe Ratio.**\n",
        "\n"
      ],
      "metadata": {
        "id": "WKzlBHBpt18F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = sp.optimize.minimize(fun=neg_sharpe_ratio_objective,\n",
        "                            x0= np.array([1/number_of_tickers for _ in range(number_of_tickers)]),\n",
        "                            args=(log_returns, covariance_matrix),\n",
        "                            method='SLSQP',\n",
        "                            bounds=tuple((0,0.3) for _ in range(number_of_tickers)),\n",
        "                            constraints=({'type': 'eq', 'fun': lambda x: np.sum(x) - 1}))\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "Xm117pdCTTQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharpe_ratio = -1 * result['fun']\n",
        "weights =  result['x']\n",
        "portfolio_return = weights @ log_returns\n",
        "portfolio_std_dev = np.sqrt(weights.T @ covariance_matrix @ weights)\n",
        "sharpe_ratio = portfolio_return / portfolio_std_dev # calculation the same sharpe ratio for all models\n",
        "\n",
        "results = pd.concat([results, pd.DataFrame([[weights @ log_returns, portfolio_std_dev, sharpe_ratio, 'Sharpe ratio optimized']], columns = results.columns)])\n",
        "print(f\"Sharpe Ratio: {sharpe_ratio:.3f}\")\n",
        "print(f\"Portfolio Return: {portfolio_return * 100:.1f} % \")\n",
        "print(f\"Portfolio Standard Deviation: {portfolio_std_dev:.3f}\")\n",
        "\n",
        "# Let's plot the Weights > 0.01\n",
        "sns.barplot(y=weights[weights >= 0.01], x = list(log_returns.index[weights >= 0.01]))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pJocI1Nytf6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare Results"
      ],
      "metadata": {
        "id": "fTfgfC69fqY8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will first add to the result the individual asset, (similar to allocating all portfolio to 1 asset)."
      ],
      "metadata": {
        "id": "UM2bVjG1K_Wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l = []\n",
        "for i in range(len(std_dev_individual_stocks)) :\n",
        "    l.append([log_returns.iloc[i], std_dev_individual_stocks.iloc[i], log_returns.iloc[i]/std_dev_individual_stocks.iloc[i], 'Individuals'])\n",
        "results = pd.concat([results, pd.DataFrame(l, columns = results.columns)])"
      ],
      "metadata": {
        "id": "4CDZDuXAnqB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results[results['Model'] == \"0.5\"]"
      ],
      "metadata": {
        "id": "oBYIKP3pK2dO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.scatterplot(results, x='Standard Deviation',  y='Return', hue = 'Model', size = 0.5)\n",
        "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G9EbBqIidDQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion on Portfolio Optimization Approaches\n",
        "- **Markowitz (Mean-Variance) Optimization**: Balances return and risk by selecting asset weights based on expected returns and correlations, offering a robust strategy but highly dependent on accurate inputs.\n",
        "- **Random Allocation**: Simple approach with no specific optimization for risk or return; often used as a baseline, but generally less efficient in managing return-risk trade-offs.\n",
        "- **Minimum Variance**: Prioritizes reducing overall portfolio risk without aiming for high returns, making it ideal for risk-averse investors; however, it may underperform in high-return environments.\n",
        "- **Sharpe Ratio Optimization**: Focuses on maximizing risk-adjusted returns, providing a balanced approach between risk and return, but sensitive to the reliability of input data on returns and risk.\n",
        "\n",
        "\n",
        "### Current Limitations\n",
        "- **Return Estimations**: Based on the previous quarter’s data, potentially misrepresenting future returns.\n",
        "- **Historical Data Constraint**: Limited to 10 years, which may not capture longer-term trends.\n",
        "- **Sector Risk Ignorance**: Lacks industry or sector-specific risk assessment, which could impact portfolio resilience.\n",
        "- **Absence of Sentiment Risk**: Ignores market sentiment, which can affect stock performance unpredictably.\n",
        "\n",
        "### Next Steps\n",
        "- **Enhanced Return Estimation**: Implement linear regression to estimate returns and conduct backtesting.\n",
        "- **Extended Data Analysis**: Use more than 10 years of historical data for a comprehensive view.\n",
        "- **Addressing Survivor Bias**: Include all stocks that were part of the S&P 500 over the last decade.\n",
        "- **Additional Risk Metrics**: Incorporate new metrics to capture industry-specific and sentiment-related risks.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Reference:**\n",
        "\n",
        "Reha H. Tutuncu(s). (2003). Advanced Lecture on Mathematical Science and Information Science I. Retrieved from [URL](https://typeset.io/pdf/advanced-lecture-on-mathematical-science-and-information-4w2ug62u0j.pdf)"
      ],
      "metadata": {
        "id": "n3w-F68BuATe"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}